{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T16:27:16.281566Z",
     "start_time": "2019-08-26T11:42:51.977850Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword_search : bawaslu\n",
      "since date [YYYY-MM-DD]: 2019-05-14\n",
      "until date [YYYY-MM-DD]: 2019-05-27\n",
      "Data bawaslu loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing tweet: 100%|█████████████████████████████████████████████████████████| 357957/357957 [02:04<00:00, 2868.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data bawaslu tweets parsed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing users: 100%|█████████████████████████████████████████████████████████| 357957/357957 [03:08<00:00, 1903.85it/s]\n",
      "location cleansing: 100%|████████████████████████████████████████████████████| 127175/127175 [4:37:00<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data bawaslu users parsed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def tweet_parser(tweets_data, since, until, path, keyword_search):\n",
    "    tweets = []\n",
    "    for tweet_obj in tqdm(tweets_data, desc='parsing tweet'):\n",
    "        result = {\n",
    "            'created_at': tweet_obj['created_at_local'],\n",
    "            'tweet_id': '_'+tweet_obj['id_str'] if tweet_obj['id_str'] != None else None,\n",
    "            'user_id': '_'+tweet_obj['user']['id_str'] if tweet_obj['user']['id_str'] != None else None,\n",
    "            'user_screenname': tweet_obj['user']['screen_name'],\n",
    "            'user_followings_count': tweet_obj['user']['friends_count'],\n",
    "            'user_followers_count': tweet_obj['user']['followers_count'],\n",
    "            'user_tweets_count': tweet_obj['user']['statuses_count'],\n",
    "            'is_verified': tweet_obj['user']['verified'],\n",
    "            'tweet_text': tweet_obj['full_text'],\n",
    "            'favorite_count': tweet_obj['favorite_count'],\n",
    "            'retweet_count': tweet_obj['retweet_count'],\n",
    "            'tweet_source': re.findall(r'\\>(.+)\\<', tweet_obj['source'])[0],\n",
    "            'place_fullname': tweet_obj['place']['full_name'] if ('place' in tweet_obj and tweet_obj['place'] != None) else None,\n",
    "            'place_name': tweet_obj['place']['name'] if ('place' in tweet_obj and tweet_obj['place'] != None) else None,\n",
    "            'place_type': tweet_obj['place']['place_type'] if ('place' in tweet_obj and tweet_obj['place'] != None) else None,\n",
    "            'place_country': tweet_obj['place']['country'] if ('place' in tweet_obj and tweet_obj['place'] != None) else None,\n",
    "            'quoted_from_tweet_id': '_'+tweet_obj['quoted_status']['id_str'] if 'quoted_status' in tweet_obj else None,\n",
    "            'quoted_from_user_id': '_'+tweet_obj['quoted_status']['user']['id_str'] if 'quoted_status' in tweet_obj else None,\n",
    "            'quoted_from_user_screenname': tweet_obj['quoted_status']['user']['screen_name'] if 'quoted_status' in tweet_obj else None,\n",
    "            'reply_to_tweet_id': '_'+tweet_obj['in_reply_to_status_id_str'] if ('in_reply_to_status_id_str' in tweet_obj and tweet_obj['in_reply_to_status_id_str'] != None) else None,\n",
    "            'reply_to_user_id': '_'+tweet_obj['in_reply_to_user_id_str'] if ('in_reply_to_user_id_str' in tweet_obj and tweet_obj['in_reply_to_user_id_str'] != None) else None,\n",
    "            'reply_to_user_screenname': tweet_obj['in_reply_to_screen_name'] if 'in_reply_to_screen_name' in tweet_obj else None,\n",
    "            'retweeted_from_tweet_id': '_'+tweet_obj['retweeted_status']['id_str'] if 'retweeted_status' in tweet_obj else None,\n",
    "            'retweeted_from_user_id': '_'+tweet_obj['retweeted_status']['user']['id_str'] if 'retweeted_status' in tweet_obj else None,\n",
    "            'retweeted_from_user_screenname': tweet_obj['retweeted_status']['user']['screen_name'] if 'retweeted_status' in tweet_obj else None,\n",
    "        }\n",
    "        # Collect original tweets\n",
    "        if (result['quoted_from_tweet_id'] == None and \n",
    "            result['reply_to_tweet_id'] == None and \n",
    "            result['retweeted_from_tweet_id'] == None):\n",
    "            result['tweet_type'] = 'tweet'\n",
    "        # Collect reply to tweets\n",
    "        elif (result['quoted_from_tweet_id'] == None and \n",
    "            result['reply_to_tweet_id'] != None and \n",
    "            result['retweeted_from_tweet_id'] == None):\n",
    "            result['tweet_type'] = 'reply'\n",
    "        # Collect quoted from tweets\n",
    "        elif (result['quoted_from_tweet_id'] != None and \n",
    "            result['reply_to_tweet_id'] == None and \n",
    "            result['retweeted_from_tweet_id'] == None):\n",
    "            result['tweet_type'] = 'quote'\n",
    "        # Collect retweeted from tweets\n",
    "        elif (result['quoted_from_tweet_id'] == None and \n",
    "            result['reply_to_tweet_id'] == None and \n",
    "            result['retweeted_from_tweet_id'] != None):\n",
    "            result['tweet_type'] = 'retweet'\n",
    "            result['retweet_count'] = 0\n",
    "            result['favorite_count'] = 0\n",
    "        # Collect retweeted from quoted tweets\n",
    "        elif (result['quoted_from_tweet_id'] != None and \n",
    "            result['reply_to_tweet_id'] == None and \n",
    "            result['retweeted_from_tweet_id'] != None):\n",
    "            result['tweet_type'] = 'retweet'\n",
    "            result['retweet_count'] = 0\n",
    "            result['favorite_count'] = 0\n",
    "        # Collect reply from quoted tweets\n",
    "        elif (result['quoted_from_tweet_id'] != None and \n",
    "            result['reply_to_tweet_id'] != None and \n",
    "            result['retweeted_from_tweet_id'] == None):\n",
    "            result['tweet_type'] = 'reply'\n",
    "        if (parse(result['created_at']) >= parse(since+' 00:00:00') and \n",
    "            parse(result['created_at']) <= parse(until+' 23:59:59')):\n",
    "            tweets.append(result)\n",
    "\n",
    "    tweets = pd.DataFrame(tweets).drop_duplicates(subset=['tweet_id']).reset_index(drop=True)\n",
    "    tweets = tweets[['created_at', 'tweet_id', 'user_id', 'user_screenname',\n",
    "                     'user_followings_count', 'user_followers_count', 'user_tweets_count',\n",
    "                     'is_verified', 'tweet_text', 'favorite_count',\n",
    "                     'retweet_count', 'tweet_source', 'tweet_type', \n",
    "                     'place_fullname', 'place_name',\n",
    "                     'place_type', 'place_country', 'quoted_from_tweet_id',\n",
    "                     'quoted_from_user_id', 'quoted_from_user_screenname',\n",
    "                     'reply_to_tweet_id', 'reply_to_user_id', 'reply_to_user_screenname',\n",
    "                     'retweeted_from_tweet_id', 'retweeted_from_user_id',\n",
    "                     'retweeted_from_user_screenname']]\n",
    "    tweets.to_csv('{}/collection_of_tweets_{}_{}-{}.csv'.format(path, \n",
    "                                                                            keyword_search, \n",
    "                                                                            since.replace('-',''), \n",
    "                                                                            until.replace('-','')),\n",
    "                       index=False)\n",
    "    return tweets\n",
    "\n",
    "def user_parser(tweets_data, since, until, path, keyword_search, GMT=7):\n",
    "    def location_lookup(loc):\n",
    "        city_look = pd.read_csv('lookup/city_lookup.csv')\n",
    "        city_look['keyword'] = city_look['keyword'].str.upper()\n",
    "        province_look = pd.read_csv('lookup/province_lookup.csv')\n",
    "        province_look['keyword'] = province_look['keyword'].str.upper()\n",
    "        subdistrict_look = pd.read_csv('lookup/sub_district_lookup.csv')\n",
    "        subdistrict_look['keyword'] = subdistrict_look['keyword'].str.upper()\n",
    "        city = 'Undefined'\n",
    "        prov = 'Undefined'\n",
    "        if loc != 'UNDEFINED':\n",
    "            for c in range(len(city_look)):\n",
    "                if city_look['keyword'][c] in loc:\n",
    "                    city = city_look['city'][c]\n",
    "                    prov = city_look['state'][c]\n",
    "                    break\n",
    "\n",
    "            if ((city == 'Undefined') & (prov == 'Undefined')):\n",
    "                for p in range(len(province_look)):\n",
    "                    if province_look['keyword'][p] in loc:\n",
    "                        city = province_look['city'][p]\n",
    "                        prov = province_look['state'][p]\n",
    "                        break    \n",
    "                        \n",
    "            if ((city == 'Undefined') & (prov == 'Undefined')):\n",
    "                for s in range(len(subdistrict_look)):\n",
    "                    if subdistrict_look['keyword'][s] in loc:\n",
    "                        city = subdistrict_look['city'][s]\n",
    "                        prov = subdistrict_look['state'][s]\n",
    "                        break\n",
    "        return [city, prov]\n",
    "    \n",
    "    users = []\n",
    "    for tweet_obj in tqdm(tweets_data, desc='parsing users'):\n",
    "        result = {\n",
    "            'created_at': tweet_obj['created_at_local'],\n",
    "            'tweet_id': '_'+tweet_obj['id_str'] if tweet_obj['id_str'] != None else None,\n",
    "            'user_id': '_'+tweet_obj['user']['id_str'] if tweet_obj['user']['id_str'] != None else None,\n",
    "            'user_screenname': tweet_obj['user']['screen_name'],\n",
    "            'user_fullname': tweet_obj['user']['name'],\n",
    "            'user_created_at': (parse(tweet_obj['user']['created_at'], ignoretz=True) + timedelta(hours=GMT)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'user_followings_count': tweet_obj['user']['friends_count'],\n",
    "            'user_followers_count': tweet_obj['user']['followers_count'],\n",
    "            'user_tweets_count': tweet_obj['user']['statuses_count'],\n",
    "            'user_location': tweet_obj['user']['location'].upper(),\n",
    "            'is_verified': tweet_obj['user']['verified']\n",
    "        }\n",
    "        if (parse(result['created_at']) >= parse(since+' 00:00:00') and \n",
    "            parse(result['created_at']) <= parse(until+' 23:59:59')):\n",
    "            users.append(result)\n",
    "        \n",
    "    users = pd.DataFrame(users).drop_duplicates(subset=['tweet_id']).reset_index(drop=True)\n",
    "    users = users.drop_duplicates(subset=['user_id']).reset_index(drop=True)\n",
    "    users_loc = []\n",
    "    for i in tqdm(range(len(users)), desc='location cleansing'):\n",
    "        loc = location_lookup(users['user_location'][i])\n",
    "        users_loc.append({'user_id': users['user_id'][i],\n",
    "                          'user_city': loc[0],\n",
    "                          'user_province': loc[1]\n",
    "                         })\n",
    "    users_loc = pd.DataFrame(users_loc)\n",
    "    users = pd.merge(users, users_loc, on='user_id')\n",
    "    users = users[['user_id','user_screenname','user_fullname','user_created_at',\n",
    "                   'user_followings_count','user_followers_count','user_tweets_count',\n",
    "                   'user_city','user_province','is_verified']]\n",
    "    users.to_csv('{}/collection_of_users_{}_{}-{}.csv'.format(path, \n",
    "                                                                     keyword_search, \n",
    "                                                                     since.replace('-',''), \n",
    "                                                                     until.replace('-','')),\n",
    "                 index=False)\n",
    "    return users\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    keyword_search = input('keyword_search : ')\n",
    "    since = input('since date [YYYY-MM-DD]: ')\n",
    "    until = input('until date [YYYY-MM-DD]: ')\n",
    "    \n",
    "    # Load Tweets Data\n",
    "    print('Load data...', end='\\r')\n",
    "    path = '{}/tweet_search_result/{}'.format(os.getcwd(), keyword_search)\n",
    "    tweets_data = []\n",
    "    for json_file in os.listdir(path):\n",
    "        if json_file.endswith('.json') and json_file.startswith('search_tweet_'):\n",
    "            tweets_data.extend([json.loads(line) for line in open('{}/{}'.format(path, json_file), errors='ignore').readlines()])\n",
    "    print('Data {} loaded!'.format(keyword_search))\n",
    "    sleep(3)\n",
    "    \n",
    "    # Parsing required fields for analysis\n",
    "    tweets = tweet_parser(tweets_data, since, until, path, keyword_search)\n",
    "    print('Data {} tweets parsed!'.format(keyword_search))\n",
    "    \n",
    "    users = user_parser(tweets_data, since, until, path, keyword_search)\n",
    "    print('Data {} users parsed!'.format(keyword_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
